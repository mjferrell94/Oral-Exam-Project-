---
title: "Oral-Exam-Paper"
format: pdf
bibliography: references.bib
editor: visual
---

```{r}
library(ggplot2)
library(readr)
library(dplyr)
Survey_Data <- read_csv(file="Tables_and_Graphs.csv")

Survey_Data |> filter(Exam == "Pre", Response %in% c("Yes","No")) |> ggplot(aes(x=Response, y=Percentage, group = Response, fill=Response))+geom_bar(stat = "identity", position=position_dodge()) + scale_y_continuous(labels=scales::percent) + facet_wrap(vars(Question_Graph_Title))

Survey_Data |> filter(Exam == "Pre", ! Response %in% c("Yes","No")) |> ggplot(aes(x=Response, y=Percentage, group = Response, fill=Response))+geom_bar(stat = "identity", position=position_dodge()) + scale_y_continuous(labels=scales::percent) + facet_wrap(vars(Question_Graph_Title))

Survey_Data |> filter(Exam == "Middle") |> ggplot(aes(x=Response, y=Percentage, group = Response, fill=Response))+geom_bar(stat = "identity", position=position_dodge()) + facet_wrap(vars(Question_Graph_Title), nrow=1)+scale_y_continuous(labels = scales::percent)

Survey_Data |> filter(Exam == "Post") |> ggplot(aes(x=Response, y=Percentage, group = Response, fill=Response))+geom_bar(stat = "identity", position=position_dodge()) + facet_wrap(vars(Question_Graph_Title), nrow=3)+scale_y_continuous(labels = scales::percent)
```



# Oral Exam Paper

## Abstract

Won't write anything here until the paper is complete

## Introduction

As instructors of statistics, we frequently find ourselves asking, "When a student answers a question, are they parroting what we've already told them, or do they truly understand?". Unfortunately, its hard to answer this question based on exam grades alone. Addressing this same concern, @Theobold2021 convincingly argued that, in line with the 2016 GAISE standards ([@carver2016guidelines]), one method that does allow for statistics and data science educators to more deeply assess what students actually know are oral exams.

Oral exams, as the name implies, are assessments where questions and answers are verbally given, with the opportunity for further probing and follow up questions. Oral exams are more commonly used outside of the United States at the undergraduate level ([@Asklund2003], [@Ramella2019]) but are still regularly used inside the US at the doctorate level. They have been used in introductory chemistry classes [@Ramella2019], math classes [@Iannone2012], introductory computer science classes [@Ohmann2019], and, of course, statistics classes [@Theobold2021].

Though, like other exams, oral exams are meant to assess, @Theobold2021 and others have argued that their benefits go beyond mere assessment. They develop communication skills ([@Joughin2010]), are a more authentic way to assess [@Wiggins1990] [@Beccaria2013], are a powerful tools in gauging a student's understanding through conversation ([@Iannone2012],[@Asklund2003], [@Huxham2012]),encourage greater preparation [@Ohmann2019], and (especially pertinent in the age of AI) greatly discourage cheating ([@Newell2023], [@Ramella2019]).

Like other forms of assessment, though, oral exams are not without their challenges, ranging from accommodating ESL learners, mitigating evaluator bias, and managing student anxiety. However, for statistics and data science educators, perhaps the biggest challenge in implementation is scale. Several studies have acknowledged the open problem scale poses to administering high quality oral exams ([@Asklund2003],[@Ohmann2019]), and the unknown effect scale would have on reliability, validity, required resources, and bias ([@Memon2010], [@kang2022providing], [@Huxham2012]). Though many studies have administered exams with an instructor to student ratio ranging from 1:10 to 1:60, what happens when that ratio becomes very large, where instructor to student ratios are over 100?

In this study, we detail our attempt at implementing oral exams at this scale, particularly in the setting of multiple large introductory statistical programming classes. Unlike other studies, which generally focus on the student experience, this study follows the graduate teaching assistants that were instructors of record. Though understanding the student experience is important, the experience of statistics and data science GTAs is of particular importance in this setting since administering and grading oral exams at a large scale is impossible without their help. Their experience and voice matters for the smooth and effective implementation of the exams.

In short, this study has two main research questions:

RQ1: What problems of practice arose for these graduate teaching assistants when attempting to scale oral exams to our large programming classes? RQ2:What recommendations can we offer as we reflect on our attempt?

## Methods

Six graduate teaching assistants that were instructors of record for one-credit programming courses (henceforth referred to as graduate student instructors) were the subjects of this study. The courses they taught covered either R programming or SAS programming. Each graduate student instructor had three or four sections of one of these courses with about 35 students per section.

### SAS Course Information

The SAS course introduces students to programming in SAS through SAS Studio in the SAS OnDemand for Academics browser-based platform. The course covers topics like reading in raw data with `PROC IMPORT`, basic row and column manipulations of SAS datasets through the `DATA step`, summarizing data numerically with `PROC FREQ`, `PROC UNIVARIATE`, and `PROC MEANS`, summarizing data graphically with `PROC SGPLOT` and `PROC SGPANEL`, analysis of means using `PROC TTEST` and `PROC GLM`, and fitting linear models (one-way ANOVA and Multiple Linear Regression) through `PROC GLM`.

The course has a prerequisite of a business statistics course or a corequisite of a second course in statistics. These restrictions allow the course to cover the creation and interpretation of statistical models without the need to introduce the statistical concepts surrounding these topics.

The course serves two major audiences:

-   Statistics majors and minors
-   Business majors

The statistics majors generally have taken an introductory statistics course, an introductory programming course in python, and are taking the second course in statistics concurrently with the course (or have already taken it). The statistics minors are similar but do not generally have the introductory programming course. The business majors usually enroll in the course having taken a business statistics course at some point earlier in their academic careers.

**Do we need the course outcomes if we have already talked about what is being taught in the course, or could we smush in the course objectives into the first paragraph?** For both audiences, the broad purpose of the course from the program level is to provide the students with a thorough introduction to using SAS in order to facilitate the use of SAS in their upper level statistics or business courses. The course learning outcomes are listed on the syllabus as follows:

> This course provides an introduction to programming using the SAS statistical software. At the end of this course students will be able to
>
> -   write and explain the purpose of basic SAS steps and statements
> -   utilize SAS help files to customize SAS programs
> -   correctly read well-structured data into SAS
> -   perform common data manipulations using SAS
> -   produce and interpret standard numeric and graphical summaries of data using SAS
> -   produce, interpret, and assess the validity of assumptions of
> -   statistical hypothesis tests and confidence intervals using SAS

### R Course Information

This course introduces students to programming in the R software through the RStudio Interactive Development Environment. The course covers topics like using and manipulating common R objects (such as lists, data frames, and (atomic) vectors), reading in raw data using the `readr`, `haven`, and `readxl` packages, creating output documents with R Markdown, common row and column manipulations with the `dplyr` and `tidyr` packages, summarizing data numerically, summarizing data graphically with the `ggplot2` package, using vectorized functions and for loops, and writing custom functions.

This course does not have a prequisite or corequisite. This causes the course to cover more programming-centric topics and less statistical topics when compared to the SAS programming course.

The course serves the same two major audiences as the SAS programming course (statistics majors/minors and business majors). Again, the statistics majors have generally had an introductory programming course prior to taking this R programming course but the other students usually do not have a programming background.

**Do we need the course outcomes if we have already talked about what is being taught in the course, or could we smush in the course objectives into the first paragraph?**The broad purpose of the course from the program level is to provide the students with a thorough introduction to using R in order to facilitate the use of R in their upper level statistics or business courses. The course learning outcomes are listed on the syllabus as follows:

> This course provides an introduction to programming using the statistical software R. At the end of this course students will be able to
>
> -   read, write, and explain the steps and purpose of R programs
> -   efficiently read in and manipulate data in R
> -   utilize R help and other resources to customize programs
> -   produce and interpret static and interactive graphics using R

### Course Structure

Both programming courses have a flipped structure, where students receive the course content prior to class in order to spend most of the time on activities involving active participation. Each class meets for 50 minutes once a week. Prior to class, students are expected to watch one to two 10-20 minute videos created by the course coordinator, as well as take two to three quizzes embedded in the content.During most class sections, instructors briefly recap material for the week, take questions, and then introduce the in-class activity. The students are then given the bulk of the time to work through the material while the graduate student instructor can check in on students and answer questions.The last five minutes of the class are used to recap important parts of the activity, while the students take another quiz a few days later.

For this iteration of the courses, there were four weeks that did not follow this structure. Two of those weeks replaced the usual in-class period with a short quiz (10 minutes) followed by time to work on a homework assignment. These assignments involved creating their own original program to answer questions of interest along with finding a data set of their own and applying class concepts. The remaining two weeks were dedicated to administering oral exams. These weeks involved no new content and no class was held. Each oral exam accounted for eight percent of the students' overall course grade.

The course format allows for graduate student instructors, even without a robust understanding of the material or effective pedagogical practices, to run the classes. During in class activities, these instructors mainly use prior programming experience and problem solving skills to appropriately guide students to successfully solve their problems. In order to manage the instructor load, all sections are capped at 40 students. Even though most sections had between 35 and 40 students, a few had much lower enrollment. In total, there were nearly 700 students across all of the sections of the two courses.

### Designing the Oral Discussions

As oral exams are not common for courses taken by these students, it was rebranded as an 'oral discussion' to hopefully lessen the anxiety students felt.

The design of the oral discussions roughly followed that of a structured interview protocol [@SIP]. In this format, subject matter experts create relevant questions and a rating scale for responses. The interviewers systematically ask these questions of the interviewees in order to be as fair as possible while ensuring valid and reliable ratings. Follow up or 'probe' questions are also created to help elicit appropriate responses at the level desired by the interviewer.

The first oral discussion occurred in week six of the R course and week seven of the SAS course. For the first oral discussion, the course coordinator developed an example program and a script of candidate questions to ask about the program. They also created a list of probable student responses with subsequent follow up questions to gain the appropriate clarity of responses. The candidate questions were split into two categories: higher-level questions involving more detailed explanations and lower-level questions involving mostly recall. As the oral discussions were slated to be taken in a five minute window, two of each type of question were developed for the first oral discussion. The grading scales were 0, 1, 2, or 3 points, and 0, 1, or 2 points for the higher-level and lower-level questions, respectively.

During a weekly meeting of all instructors, the instructors and course coordinators went through the example programs (one SAS program and one R program) and questions written by the course coordinator. The graduate student instructors then split into pairs (one a SAS instructor and one an R instructor) and practiced administering the discussion. Using this as feedback, the questions and follow ups were modified as needed.

For the second oral discussion, occuring in week 12 for both courses, the course coordinator developed an example program to share with students and a similar program to use for the actual oral discussion. This time a weekly meeting was devoted to having the graduate student instructors develop questions and the corresponding follow ups. The graduate student instructors were again split into pairs and practiced administering the discussions.

After the first oral discussions, the graduate student instructors felt strongly that they would like to have a finer scale for their grading rubric. Whereas before the lower-level questions were out of 0, 1, or 2 scale, these were now out of a 0, 1, ..., 4 scale. Similarly, the higher-level questions were now graded on a scale of 0, 1, ..., 6.

The oral discussion scripts used by the graduate student instructors and SAS or R programs are available in the appendix.

### Administering the Oral Discussions

The oral discussions were administered through zoom. This decision was made for the ease of scheduling for both the instructors and the students, since both groups each had their own class schedules and finding a time to schedule out rooms that would work for everyone wasn't feasible. The discussions were set to five minutes in length with the intention that the actual time for the discussions would be closer to three-four minutes, allowing for a buffer between appointments. The discussions were closed notes and did not involve students coding. They were instructed to be prepared to discuss code only. Instructors were encouraged to build in down time for their appointments to deal with any appointments that went long or had technical difficulty. For instance, only making appointments available from the start of the hour until 50 minutes past. This would leave 10 minutes to account for issues that may arise.

The weeks prior to the discussions students were able to use an online scheduler to sign up for a five minute time slot. They were instructed to sign up for a time slot by the Wednesday prior to discussion week and contact their instructor if none of the designated time slots worked for them. If this wasn't followed, a 25% deduction to their score could be given (this did not end up being used for any students). Students with accommodations were to contact their instructor to make sure they were able to have their accommodations met.

The week prior to the oral discussion, students were provided an example program, the rubric used for grading the discussions, and the instructions for how the oral discussion would progress. For the first oral discussion, we chose to use the same program as both the example program and the actual program used in the oral discussion in order to ease student anxiety. As students were more comfortable in the second round of oral discussions, different programs were used. The rubric used was based on that given by [@Theobold2021], and was modified between the first discussion and second discussion based on instructor feedback. The instructions for the discussion itself are given below **should the instructions be in the appendix??**:

-   We want to ask you a few questions about a SAS program (or an R Markdown document) we'll share with you.
-   I'll share my screen, ask you to consider particular pieces of code and describe to me what that code does or why we might run it.
-   I may ask clarification questions or follow-up questions if you don't fully answer the question.
-   If you don't know the answer, that's ok. Just let us know and we'll move to the next item.
-   We do have firm time limits on answers to questions. We may have to cut you off so we can get all of the questions in a timely manner.
-   Any questions?

Students were instructed to log into the zoom meeting a few minutes prior to their designated time slot. The zoom meetings themselves utilized the waiting room feature. This ensured that students would not pop into the meeting and interrupt an ongoing discussion. Students were required to have their camera on during the discussions. Students are not required to have a laptop at this institution but no instructors reported issues with students not being able to accommodate virtual meetings with a webcam (this may be due to the excellent library facilities on campus).

The rubric for the oral discussion was set up on the learning management system allowing the graduate student instructors to provide their graded feedback as the discussion were taking place. These grades were not released until all students had completed their discussions.Along with this, the discussions themselves were recorded so that any disagreements about grading between the graduate student instructors and students could be mediated by the course coordinator. Only two disagreements required mediation across all of the administered oral discussions.

Lastly, some graduate student instructors found that they needed to have extra 'hidden' times on the last day of the week to account for students that had missed their appointments due to personal or technological reasons. The number of students needing this extra period was pretty low (five to ten) for each graduate student instructor.

In total, three graduate student instructors had four sections and administered oral exams to roughly 140 students each. Three graduate student instructors had three sections each but these tended to be sections with slightly lower enrollment. Two of these graduate student instructors administered oral exams to roughly 90 students. One graduate student instructor had a teaching assistant appointment for less time so only administered exams to two of their sections (roughly 70 students). The course coordinator administered oral exams to the remaining section of students.

### Monitoring Instructor Sentiment and Issues

In order to understand problems that arose, and sentiment around, administering the oral discussions, the course coordinator requested feedback at various points of the process formally and informally.

**I think providing a little visual flow diagram of the feedback items would make it easier for people to see the timing of items. Giving them names used for easy reference later would also likely be good. Draw.io**

Initial thoughts about giving, and experience with, oral exams were requested on a Google form prior to administering the discussions. The R course had its first oral discussion one week prior to the SAS course. Reflections and advice were discussed in the weekly meeting and recorded for future reference. The meeting and conversations after the first SAS course oral discussion was likewise recorded. Another Google form was administered at this point as well. The second oral discussions occurred during the same week for both courses. The meeting and conversations the week after administration were recorded. Lastly, a Google form was given to record their final thoughts on administering oral discussion in the course. All of the forms are available in the appendix.

## Data Analysis: Analyzing Instructor Sentiment and Issues

The most common approach used to analyze qualitative data involves iteratively going through the data, creating codes, and then grouping pieces of coded data into groups (usually called "themes"). This was the approach taken to analyze our survey and meeting data, formally called thematic analysis ([@zhang2009qualitative]).

More specifically, in the first iteration, data was coded based on the use of particular words (such as "time" or "technology") and also on whether or not it came from a survey or a discussion. Pieces of text were then grouped together manually to create five starting themes. In the second and third iterations, these themes were refined after recognizing that they may be better represented by including more sub-themes; for example, instead of discussion of time being a theme, breaking this down into discussions of the quantity and quality of time. Finally, as we passed through the data, we looked over which themes had sufficient support to be included. Themes were included only if they had 5 or more pieces of evidence.

Along with the themes generated from the open ended responses on surveys and whole instructor discussions, closed ended responses on surveys were also tabulated. This included counting the different number of responses to questions such as "Have you ever given an oral exam before?" on our pre-exam survey or "Would a moodle quiz have accomplished the same thing \[as an oral exam\]?" on our post-exam survey.

**I think first we should just give a quick summary about the 'easy' stuff included on the first google form. Have they taken an oral exam before, their comfort level before and after. That would be easily described in a couple of tables and help people ground their understandings of where the instructors were at the beginning of the process. Then summarize the info from the last google form. After that, then we dive into the open ended feedback via what you do below. Get data from google forms**

```{r}




```




## Results (I think this section would benefit from having the items in the themes/sub-themes separated out into pre-oral exams (discussions), between the two discussions, and after both discussions (however you want to reference each of those items). This would give some clarity on what was 'fear' based :) and what was a experience based :) Make each theme time based. 

### Closed Ended Survey Responses

### Open Ended Survey Responses and Instructor Discussions

#### Theme 1: The graduate student instructors focused on time

**Sub-theme 1: Instructors focused on the quantity of time**

On a pre-exam survey, half of the graduate student instructors expressed concerns about the "time" or "timeframe" of administering the oral discussions to such a large number of students in just a week period. After the first round of oral discussions, some of the graduate student instructors gave estimates of 12 to 18 hours to complete their oral discussions (their graduate teaching assistantship appointments were for 20 hours a week).

Others used verbal descriptions, such as it "took way too long" and "overall, very time consuming" to express the magnitude of the time commitment. Words such as "grueling", that this was something they had to "endure", and that it was "extremely time inefficient" were used to express their feelings towards the amount of time needed. One graduate student instructor, after administering their first oral exam, said (somewhat jokingly), "I was losing my sanity being on zoom for so long".

One reason for this focus on the time commitment was given by one graduate student instructor, who commented that they were promised less work as a first-year graduate student, but that this was not the case in this class."If the oral exam format goes unchanged", they said, "I am unsure if I am fit to be an instructor next semester."

**Sub-theme: What instructors were doing during that time**

Aside from the actual time it took to administer the exams, there was talk centered around what they could and couldn't do during their schedule oral discussion times. In the first whole group meeting after the first oral discussion, several people joked about not even being able to use the bathroom since you have to be "100% focused". In a later discussion, they focused on incorporating time to get a snack, stand up, and walk around. **(Perhaps a note here that many instructors did not follow the recommendation about leaving time in their schedules?)**

The way they were spending their time administering exams was contrasted with grading done on homework assignments administered in the class. In one discussion, a graduate student instructor commented that, for them, the exams took longer, and, unlike with homework grading, for the exams you had to "block out time". Another graduate student instructor disagreed, saying the time commitment felt similar, but later, acknowledging that it could take similar time. Another graduate student instructor also commented that grading homework was more "flexible".

**Sub-theme: When things went wrong with timing**

Especially after administering the first oral discussion, the graduate student instructors focused on issues with student timing. One talked about the fragility of the online set up, where if even one student comes late, it can "ruin the timing for all of the students". This inspired another strong question about "how to make everyone register and show up on time". Having to reschedule with students who missed, either to Saturday or to the next week, and how to avoid it, was a large talking point in the final group discussion after the administration of the last oral discussion. Other issues of timing were caused by technical difficulties with Zoom and the ensuing delays, as well as the burden of time bleeding into other classes and exams the graduate student instructors had outside of their positions.

When the graduate student instructors were asked about advice they would give to future graduate student instructors participating in the role, their advice overwhelmingly centered around optimal time management due to previously mentioned experiences with students not showing up on time or having to reschedule. For example, graduate student instructors offered comments such as "If possible, avoid scheduling any time on Friday" so as to plan for students who miss their time slots, "try to make everyone schedule and show up on time", and "build in more than a 5 minute break every 30 minutes" or else, due to students not arriving to the meeting on time, "you'll need to use \[it\]".

#### Theme 2: The graduate student instructors focused on equity

Even before administering exams, the graduate student instructors worried about their role not just as administrators but as graders. One worried that the grading might be "too subjective". Another worried about their "poor memory" and bias that could come from familiarity. One graduate student instructor even cautioned against "food bias" where you may grade someone more harshly while hungry. Some of these issues were borne out during the first discussion when two graduate student instructors used different follow-up questions other than on the rubric when asking about an R object, and the ?teacher? **if you are referring to me here, I didn't grade any of the R ones...** and one graduate student instructor used different follow up questions to discuss R libraries. Several graduate student instructors expressed interest in changing the grading scale after the first oral discussions so as to be fair to students who may have needed prompting, but still got the right answer.

The graduate student instructors were also worried that the oral exam would be confounded with speaking ability. In one exchange between the course coordinator and one graduate student instructor, the instructor shared one ESL student wanted to do it in person since, through zoom, "it \[was\] hard for them to understand the questions clearly". Other graduate student instructors expressed similar views on the first survey, with one on the final survey indicating that ESL students should be given more time, since "this is not an English test".

#### Theme 3: Their views on oral vs written exams

There were other minor focuses that didn't always consistently appear. One was their views on oral vs written exams. After the first round of oral discussions, a few graduate student instructors expressed doubt over the utility of the oral discussion format. "Not sure what this accomplishes over HW/\[online\] quiz" one said. "A paper quiz", another said, "could serve the same goal". After seeing these responses, a specific question about their views on oral vs written exams was asked, with mixed responses. Half agreed there was something different or better, but those differences boiled down to "open book" vs "closed book" exams, ?**and?** examining "on the fly knowledge". One graduate student instructor did mention the benefit of being able to "follow up". The other half thought that a well-crafted, open ended quiz could essentially serve the same purpose.

#### Theme 4: Positives of the oral exam experience

When asked directly about positives, half of the graduate student instructors responded. All of the comments were centered around having increased student interactions where, in such a large class, they usually didn't. One graduate student instructor commented that, unprovoked, a few students talked about liking the format.

## Discussion

**Matthew and Justin** will work on this part after literature review and results are complete.

To the researchers' knowledge, this is the first large scale oral exam study done in the statistics and data science education literature and the first to examine the attitudes and experiences of novice instructors being guided by a seasoned instructor.

Unlike previous studies, this study had **this many participants**. However, it involved conducting oral exams using realistic resources, necessitating the reduction of oral exam times (5 minutes) and increasing the number of instructors (six graduate student instructors and one full-time professor). Since in the large scale scenario graduate student instructors were essential to successful implementation of an oral exam at scale, we focused on their thoughts and feelings.

From their perspective, the oral exam format was probably a failure. Most of the topics centered around being able to survive administering them, as well as questioning their effectiveness in comparison to more standard assessments. **Students see written exam as gold standard citation?**. *By limiting the time allocated to each student, it seemed the graduate student instructors didn't perceive the benefits usually seen by the inclusion of such assessments.*? This differed from the course coordinators feelings and experiences in administering the oral exams. They found that, especially in the second round of exams when students were more comfortable with the format, the short meetings were an excellent and refreshing way to assess students knowledge. We hypothesize that part of this difference in feelings may come from the novice instructors discomfort with assessing students generally, and especially assessing them on the fly in real time. The lower pressure grading of quizzes and homework assignments give more time to be sure of answers given and deem what is an appropriate score should be.

On a positive note, these graduate student instructors were aware of and sensitive to potential biases and equity issues, and had an excellent discussion of ways to attempt to mitigate these problems.

Overall, we would suggest that under realistic circumstances, administering oral exams at larger scales may greatly weaken what makes oral exams important/interesting/powerful in the first place. With the necessary shortening of their administration comes a loss in reliability ([@Memon2010]).

**Justin is going to stop here and wait for feedback on the previous parts!** **Thanks for the excellent work on this Matt!**

Probably will mention this not being a good idea, probably losing what makes an Oral exam really good as we try to adapt to scale

Probably going to talk about how this is our attempt in a realistic setting....we could try to create an ideal one, and see, or we could couch it in based on the literature, ideally it would look like this. See that it's really not possible, this was our attempt realistically.

One big issue is most oral exams are in a problem based format, some questions, time....with only 5 minutes, you can't do that or do that well. You lose that part of really testing a student's thinking and assessing how well they can explain *and* do.

Issues with reliability.

**Future research questions** include

-   AI enhanced oral assessment?
-   Student perceptions positive in literature, but how does doing it for 5 minutes in zoom change things? Having to compare and contrast experiences
-   **Others?**

## Conclusion

**Matthew and Justin** will work on this part after literature review and results are complete.

## References
