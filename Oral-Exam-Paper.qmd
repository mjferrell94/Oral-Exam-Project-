---
title: "Oral-Exam-Paper"
format: pdf
bibliography: references.bib
editor: visual
---

# Oral Exam Paper

## Abstract

Won't write anything here until the paper is complete

## Introduction

-   We are going to summarize Allison's paper, while giving more support to some of her claims with the advantages of oral exams.

How do you know your students understand what's being taught? When they score high on your exams, does that mean they really understand? Following this train of thought, Theobold (2021) ([@Theobold2021]) convincingly argued that, in line with the 2016 GAISE standards that recommend a focus on conceptual understanding and assessment of that understanding [@carver2016guidelines], oral exams allow for us as statistics and data science educators to more deeply assess what our students actually know.

The benefits of oral exams go beyond assessment. As argued by Theobold (2021) and others, they also develop communication skills \[[@Joughin2010]\], are a more authentic way to assess [@Wiggins1990] [@Beccaria2013], are powerful tools in gauging a student's understanding and conceptions through conversation ([@Iannone2012],[@Asklund2003], [@Huxham2012]), and (especially pertinent in the age of AI Chatbots) they greatly discourage plagiarism ([@Newell2023], [@Ramella2019]). Students also prepare more? (\*This is interesting... I wonder if that kind of thing has been looked at. How do students differ in their preparation for an oral exam vs a pen/paper style.)

Oral exams, of course, are not without their challenges. For example, whereas a written assessment assumes a student's proficiency with the *written* language, oral exams assume a student's proficiency with the *spoken* langue. ESL students in particular may experience difficulties unrelated to the content being tested, potentially turning a statistics test into an English test. Another issue is bias: Written exams can have confusing questions, but at least they can be graded blind. Oral exams, on the other hand, require a face to face interaction, and with that brings the possibility of evaluating a student based on how they've done previously, ..... and even feeling hungry (as one TA mentions later) need to be carefully evaluated. Finally, oral exams may not only negatively impact our students with test anxiety (which a written exam also might), but the new format may make that anxiety worse, along with pulling in our students with social or speaking anxiety.

These challenges have been successfully mitigated in the literature, but only for smaller classes. How do we do an assessment with promising returns on understanding our students' understanding in a class of 100? 200? Or, in the case of this study, 884? This is an open problem in the oral exam literature [@Asklund2003],[@Ohmann2019]. As class sizes increase, it becomes necessary to enlist help in the form of other instructors, potentially increasing bias in evaluation. Oral exams that, normally, range from 15-45 minutes long **citations** may need to be shortened, which will impact reliability negatively [@Memon2010]. Where professors could administer these exams in person, they may instead need to administer them on a computer, using video calling software like zoom, which can enlist a host of new issues to deal with.

With scale comes a new group to study beyond the students: the administrators of the exam. This study follows a group of graduate teaching assistants who are the instructor of record for one-credit programming courses. While seasoned instructors may feel comfortable engaging in a conversation with students and gauging understanding, novice instructors administering oral exams is a very different proposition. This study attempts to understand problems of practice that arise for novice instructors when attempting to scale oral exam administration to a large number of students.

In addition, we outline the process used to develop and administer oral exams at scale with a discussion of what worked well and what could be improved upon.

## Methods

This study follows six graduate teaching assistants that were instructors of record for one-credit programming courses. These courses cover either R programming or SAS programming. Each instructor had three or four sections of one of these courses.

### SAS Course Information

This course introduces students to programming in SAS through SAS Studio in the <a href = "https://www.sas.com/en_us/software/on-demand-for-academics.html" target = "_blank">SAS OnDemand for Academics</a> browser-based platform. The course covers topics like reading in raw data with `PROC IMPORT`, basic row and column manipulations of SAS datasets through the `DATA step`, summarizing data numerically with `PROC FREQ`, `PROC UNIVARIATE`, and `PROC MEANS`, summarizing data graphically with `PROC SGPLOT` and `PROC SGPANEL`, analysis of means using `PROC TTEST` and `PROC GLM`, and fitting linear models (one-way ANOVA and Multiple Linear Regression) through `PROC GLM`.

The course has a prerequisite of a business statistics course or a corequisite of a second course in statistics. These restrictions allow the course to cover the creation and interpretation of statistical models without the need to introduce the statistical concepts surrounding these topics.

The course serves two major audiences:

-   Statistics majors and minors
-   Business majors

The statistics majors generally have taken an introductory statistics course, an introductory programming course in python, and are taking the second course in statistics concurrently with the course (or have already taken it). The statistics minors are similar but do not generally have the introductory programming course. The business majors usually enroll in the course having taken a business statistics course at some point earlier in their academic careers.

For both audiences, the broad purpose of the course from the program level is to provide the students with a thorough introduction to using SAS in order to facilitate the use of SAS in their upper level statistics or business courses. The course learning outcomes are listed on the syllabus as follows:

> This course provides an introduction to programming using the SAS statistical software. At the end of this course students will be able to
>
> -   write and explain the purpose of basic SAS steps and statements
> -   utilize SAS help files to customize SAS programs
> -   correctly read well-structured data into SAS
> -   perform common data manipulations using SAS
> -   produce and interpret standard numeric and graphical summaries of data using SAS
> -   produce, interpret, and assess the validity of assumptions of
> -   statistical hypothesis tests and confidence intervals using SAS

### R Course Information

This course introduces students to programming in the R software through the RStudio Interactive Development Environment. The course covers topics like using and manipulating common R objects (such as lists, data frames, and (atomic) vectors), reading in raw data using the `readr`, `haven`, and `readxl` packages, creating output documents with R Markdown, common row and column manipulations with the `dplyr` and `tidyr` packages, summarizing data numerically, summarizing data graphically with the `ggplot2` package, using vectorized functions and for loops, and writing custom functions.

This course does not have a prequisite or corequisite. This causes the course to cover more programming-centric topics and less statistical topics when compared to the SAS programming course.

The course serves the same two major audiences as the SAS programming course (statistics majors/minors and business majors). Again, the statistics majors have generally had an introductory programming course prior to taking this R programming course but the other students usually do not have a programming background.

The broad purpose of the course from the program level is to provide the students with a thorough introduction to using R in order to facilitate the use of R in their upper level statistics or business courses. The course learning outcomes are listed on the syllabus as follows:

> This course provides an introduction to programming using the statistical software R. At the end of this course students will be able to
>
> -   read, write, and explain the steps and purpose of R programs
> -   efficiently read in and manipulate data in R
> -   utilize R help and other resources to customize programs
> -   produce and interpret static and interactive graphics using R

### Course Structure

These programming courses have a flipped structure where students receive the course content prior to their class meeting time in order to spend most of the class time on activities involving active participation. With many of the students in the courses having limited to no programming exposure, having a flipped course allows the students to gain basic knowledge of the material through videos. They can then come to class and actually **do** programming while having an expert (the course instructor) in the room and support from their peers. This ideally leads to students having a direct line of help when they get stuck rather than running into an error, getting frustrated, and stopping their learning. They also can see the struggles of others to know that they aren't out of place if they are having difficulty. Not only that but students can go back to rewatch topics they find difficult.

Although not the purpose of using a flipped structure, another benefit of this design is that graduate students can reasonably be expected to support students within the classroom as the instructor of the course. These graduate students may not have the pedagogical knowledge or experience to effectively teach programming material themselves, but many have the problem solving skills to help students work through issues that come up. As these two courses serve approximately 700 students each semester, this is a definite benefit from the administrative side. Graduate students can be assigned to reasonably small sections of students (40 students maximum) and provide more one-on-one attention to the learners.

These classes meet once a week for 50 minutes at a time. Most weeks students are expected to watch one to two videos that cover the learning objectives for the week. These videos are approximately 10-20 minutes each and are created by the course coordinator. The videos usually have two or three quizzes throughout, each with three to four questions, to help students assess their understanding as they watch. Students are expected to complete these videos prior to their weekly session.

During most class periods, the instructor briefly recaps the material for the week (less than five minutes), takes questions, and introduces the in-class activity for the week. The students are then given the bulk of the class time to work in small groups or individually on the activity. The instructor takes an active role during this period, checking in on students and answering questions that come up. The last five minutes of the class are used to recap important parts of the activity.

Weeks that follow this structure then have a quiz administered on the course learning management software that must be completed a few days after the class. The questions in these quizzes vary across the common auto-graded types of questions such as multiple choice, fill in the blank, etc. and some are very similar to the questions asked during the videos.

There were four weeks that did not follow this structure. Two of those weeks replaced the usual in-class period with a short quiz (10 minutes) followed by time to work on a homework assignment. These assignments involved creating their own original program to answer questions of interest along with finding a data set of their own and applying class concepts.

The remaining two weeks were dedicated to administering oral exams. These weeks involved no new content and no class was held. Each oral exam accounted for eight percent of the students' overall course grade.

Each section of the course had a maximum of 40 students. Most sections had between 35 and 40 students but a few had much lower enrollment. In total, there were nearly 700 students across all of the sections of the two courses. Three instructors had four sections and administered oral exams to roughly 140 students each. Three instructors had three sections each but these tended to be sections with slightly lower enrollment. Two of these instructors administered oral exams to roughly 90 students. One instructor had a teaching assistant appointment for less time so only administered exams to two of their sections (roughly 70 students). The course coordinator administered oral exams to the remaining section of students.

### Designing the Oral Discussions

As oral exams are not common for courses commonly taken by these students, the oral exam was rebranded as an 'oral discussion' to hopefully lessen the anxiety students felt.

The design of the oral discussions roughly followed that of a structured interview (reference). The goal of using this format was to make the discussions as similar as possible so as to be as fair as possible to students being evaluated by different instructors. In this structure, a interviewers read questions in order from a script. Follow up questions are designed around common answers that may be slightly lacking. (Need more on this I think.)

For the first oral discussion, the course coordinator developed the example program and developed a script of candidate questions with possible student responses and subsequent follow up questions. These questions were split into two categories: questions involving explanation and questions not involving explanation. Two of each type of question were developed for the first oral discussion. During a weekly meeting of all instructors, the instructors and course coordinators went through the example programs (one SAS program and one R program) and questions with follow up that were developed by the course coordinator. The instructors then split into pairs (one a SAS instructor and one an R instructor) and practiced administering the discussion to the other instructor. Using this as feedback, the questions and follow ups were modified.

Between the first and second oral discussions, feedback about the rubric led to refining the rubric to have more values (i.e. 0, 1, 2, 3 to 0, 1, 2, ..., 6) as instructors felt they were able to discriminate between a finer scale.

For the second oral discussion, the course coordinator developed an example program to share with students and a similar program to use for the actual oral discussion. This time a weekly meeting was devoted to having the instructors of the courses develop questions and the corresponding follow ups. The instructors were again split into pairs and practiced administering the discussions.

The scripts for the first and second discussions are available in the appendix.

### Administering the Oral Discussions

The oral discussions were administered through zoom \*(we should probably talk about why we chose to use zoom over being in person.)\* The discussions were set to five minutes in length with the intention that the actual time for the discussions would be closer to three-four minutes, allowing for a buffer between appointments. The discussions were closed notes and did not involve students coding. They were instructed to be prepared to discuss code only.

The week prior to the discussions students were able to use an online scheduler to sign up for a five minute time slot. They were instructed to sign up for a time slot by the Wednesday prior to discussion week and contact their instructor if none of the designated time slots worked for them. If this wasn't followed, a 25% deduction to their score could be given (this did not end up being used for any students). Students with accommodations were to contact their instructor to make sure they were able to have their accommodations met.

The week prior to the oral discussion, students were provided an example program, the rubric for used for grading the discussions, and the instructions for how the oral discussion would progress. For the first oral discussion, we chose to use the same program as both the example program and the actual program used in the oral discussion in order to ease student anxiety. As students were more comfortable in the second round of oral discussions, different programs were used. Please see the appendix for all programs used. The rubric - also provided in the appendix - was based on [@Theobold2021], and was modified between the first discussion and second discussion based on instructor feedback. The instructions for the discussion itself were:

-   We want to ask you a few questions about a SAS program (an R Markdown document) we'll share with you.
-   I'll share my screen, ask you to consider particular pieces of code and describe to me what that code does or why we might run it.
-   I may ask clarification questions or follow-up questions if you don't fully answer the question.
-   If you don't know the answer, that's ok. Just let us know and we'll move to the next item.
-   We do have firm time limits on answers to questions. We may have to cut you off so we can get all of the questions in a timely manner.
-   Any questions?

Students were instructed to log into the zoom meeting a few minutes prior to their designated time slot. The zoom meetings themselves utilized the waiting room feature. This ensured that students would not pop into the meeting and interrupt an ongoing discussion. Students were required to have their camera on during the discussions.

The discussions themselves were recorded so that any disagreements about grading between the instructors and students could be mediated by the course coordinator.

### Monitoring Instructor Sentiment and Issues

In order to understand problems that arose, and sentiment around, administering the oral discussions, the course coordinator requested feedback. Conversations about administering the discussions were noted prior to first oral discussion given in the SAS course. This discussion occurred two weeks prior to the first discussion in the R course. Feedback from the

-   How was the exam monitored?
    -   Weekly meetings
    -   Surveys

**Matthew** will work on the methodology of analyzing the data.

This is more of a descriptive thematic analysis. We want to find themes, or patterns, in what is being said, but we're not trying to interpret, just describe what the TAs are focusing on. Three passes. Multiple passes were made over the surveys and discussion comments. The first few passes were to begin clustering comments and discussion points by common themes. The next few passes were to iteratively update the themes that had been created in order to refine the themes. Time split up. Um....where we compiled evidence and time points and where they came from underneath them, then finding evidence for each one or splitting them.

## Results

### The TAs focused on time

**The Quantity of time**

On a pre-exam survey, half of the TAs expressed concerns about the "time" or "timeframe" of administering so many oral exams to so many students over a week. After the first round, some of the TAs gave estimates of 12 to 18 hours to complete their oral exams.

Others used verbal descriptions, such as it "took way too long" and "overall, very time consuming" to express the magnitude of the time commitment. Words such as "grueling", that this was something they had to "endure", and that it was "extremely time inefficient" were used to express their feelings towards the amount of time needed. One TA, after administering their first oral exam, said (somewhat jokingly), "I was losing my sanity being on zoom for so long".

One reason for this focus on the time commitment was given by one TA, who commented that they were promised less work as first years, but that this was not the case in this class."If the oral exam format goes unchanged", they said, "I am unsure if I am fit to be an instructor next semester".

**What they were doing during that time**

Beyond the actual time it took to administer the exams was talk centered around what they could and couldn't do during that time. In the first whole group discussion after the first oral exam, several people joked about not even being able to use the bathroom since you have to be "100% focused". In a later discussion, they focused on incorporating time to get a snack, stand up, and walk around.

The way they were spending their time administering exams was contrasted with HW grading. In one discussion, one TA commented that, for them, the exams took longer, and, unlike HW, with the exams you had to "block out time". Another TA disagreed, saying the time commitment felt similar, but later, acknowledging that it could take similar time, a final TA commented that grading HW was more "flexible".

**When things went wrong with timing**

Especially after the first few exams, TAs focused on issues with student timing. One talked about the fragility of the online set up, where if even one student comes late, it can "ruin the timing for all of the students". This inspired another strong question about "how to make everyone register and show up on time". Having to reschedule with students who missed, either to Saturday or to next week,and how to avoid it, dominated the final group discussion after the administration of the last oral exam. Other issues of timing were caused by technical difficulties with Zoom and the ensuing delays, as well as the burden of time bleeding into other classes and exams the TAs had outside of their positions.

When the TAs were asked about advice they would give to future TAs participating in the same experience as themselves, their advice overwhelmingly centered around optimal time management due to previously mentioned experiences with students not showing up on time or having to reschedule. For example, TAs offered comments such as "If possible, avoid scheduling any time on Friday" so as to plan for students who miss their time slots, "try to make everyone schedule and show up on time", and "build in more than a 5 minute break every 30 minutes" or else, due to students not arriving to the meeting on time, "you'll need to use \[it\]".

### The TAs focused on Equity

Even before administering exams, TAs worried about their role not just as administrators but as graders. One worried that the grading might be "too subjective". Another worried about their "poor memory" and bias that could come from familiarity. One TA even cautioned against "food bias" where you many grade someone more harshly while hungry. Some of these issues were borne out during the first discussion when two TAs used different follow-up questions other than on the rubric when asking about an R object, and the teacher and one TA used different follow up questions to discuss R libraries. Several TAs after the first few oral exams expressed interest in changing the grading scale, so as to be fair to students who may have needed prompting, but still got the right answer.

The TAs were also worried that the oral exam would be confounded with speaking ability. In one exchange between the teacher and one TA, the TA shared one ESL student wanted to do it in person since, through zoom, "it \[was\] hard for them to understand the questions clearly". Other TAs during the first survey expressed similar views, with one on the final survey indicating that ESL students should be given more time, since "this is not an english test".

There were other minor focuses that didn't always consistently appear or only appeared because a they were explicitly asked in a survey: their views on oral vs written exams, and positive thoughts (if any) about the process.

### Their views on Oral vs Written Exams

After the first round of exams, a few TAs expressed doubt over the utility of the oral exam format. "Not sure what this accomplishes over HW/\[online\] quiz" on said. "A paper quiz", another said, "could serve the same goal". After seeing these responses, a specific question about their views on oral vs written exams was asked, with mixed responses. Half agreed there was something different or better, but those differences boiled down to "open book" vs "closed book" exams, examining "on the fly knowledge". Luckily one TA did mention the benefit of being able to "follow up". The other half thought that a well crafted, open ended quiz could essentially serve the same purpose.

### Positives or Oral exam experience

When asked directly about positives, only half of the TAs had something to say, with all the comments being centered around having increased student interactions where, in such a large class, they usually didn't. One TA commented that, unprovoked, a few students talked about liking the format.

## Discussion

**Matthew and Justin** will work on this part after literature review and results are complete.

Probably will mention this not being a good idea, probably losing what makes an Oral exam really good as we try to adapt to scale

Probably going to talk about how this is our attempt in a realistic setting....we could try to create an ideal one, and see, or we could couch it in based on the literature, ideally it would look like this. See that it's really not possible, this was our attempt realistically.

One big issue is most oral exams are in a problem based format, some questions, time....with only 5 minutes, you can't do that or do that well. You lose that part of really testing a student's thinking and assessing how well they can explain *and* do.

Issues with reliability.

**Future research questions** include

-   AI enhanced oral assessment?
-   Student perceptions positive in literature, but how does doing it for 5 minutes in zoom change things? Having to compare and contrast experiences
-   **Others?**

## Conclusion

**Matthew and Justin** will work on this part after literature review and results are complete.

Trying out references [@asklund2003oral].

More [@baghdadchi2022exploratory]

## References

Other stuff moved:

Really we can acknowledge the perfect situation. Show what our situation was like. How much of an impact did that make? We don't know for sure... we only know it in terms of

First, we are attempting it. Second, we're focusing on a TA perspective. As oral exams scale, help is required, and in a university setting is going to come from less experienced instructors. We want to see what we learned from this, and also, for less experienced instructors engaging in a large scale oral exam process, their thoughts and feelings that had compared to what we might envision as a team leader. t have wanted. ....I still need more clarification here haha (**Justin might be good here**).

-   What about very large ones?

-   Talk about how this is an open issue for stats educators and of course the wider oral exam literature as a whole, and what issues scale might introduce

-   Our contribution is sharing how we attempted in a statistical programming setting implementing oral exams at large scale

-   **Why are we Focusing on the TA perspective since they were the ones mainly administering it?**
