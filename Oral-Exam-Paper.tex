% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Oral-Exam-Paper},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Oral-Exam-Paper}
\author{}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[enhanced, boxrule=0pt, sharp corners, interior hidden, borderline west={3pt}{0pt}{shadecolor}, breakable, frame hidden]}{\end{tcolorbox}}\fi

\hypertarget{oral-exam-paper}{%
\section{Oral Exam Paper}\label{oral-exam-paper}}

\hypertarget{abstract}{%
\subsection{Abstract}\label{abstract}}

Won't write anything here until the paper is complete

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

How do you know your students understand what's being taught? When they
score high on your exams, does that mean they really understand?
Following this train of thought, Theobold (2021) convincingly argued
that, in line with the 2016 GAISE standards that recommend a focus on
conceptual understanding and assessment of that understanding ((Carver
et al. 2016)), oral exams allow for us as statistics and data science
educators to more deeply assess what our students actually know.

The benefits of oral exams go beyond assessment. As argued by Theobold
(2021) and others, they also develop communication skills ((Joughin
2010)), are a more authentic way to assess (\textbf{Wiggins1990?})
(Beccaria 2013), are powerful tools in gauging a student's understanding
and conceptions through conversation ((Iannone and Simpson
2012),(Asklund and Bendix 2003), (Huxham, Campbell, and Westwood 2012)),
and (especially pertinent in the age of AI Chatbots) they greatly
discourage plagiarism ((Newell 2023), (Ramella 2019)). Students also
prepare more? (Yes, add this in.)

Oral exams, of course, are not without their challenges. For example,
whereas a written assessment assumes a student's proficiency with the
\emph{written} language, oral exams assume a student's proficiency with
the \emph{spoken} langue. ESL students in particular may experience
difficulties unrelated to the content being tested, potentially turning
a statistics test into an English test. Another issue is bias: Written
exams can have confusing questions, but at least they can be graded
blind. Oral exams, on the other hand, require a face to face interaction
\textbf{(in-person or virtually - might reword to include that
somehow)}, and with that brings the possibility of evaluating a student
based on how they've done previously, \ldots.. and even feeling hungry
(as one TA mentions later) need to be carefully evaluated. Finally, oral
exams may not only negatively impact our students with test anxiety
(which a written exam also might), but the new format may make that
anxiety worse, along with pulling in our students with social or
speaking anxiety.

These challenges have been succes

fully mitigated in the literature, but only for smaller classes. How do
we do an assessment with promising returns on understanding our
students' understanding in a class of 100? 200? Or, in the case of this
study, 884? This is an open problem in the oral exam literature (Asklund
and Bendix 2003),(Ohmann 2019). As class sizes increase, it becomes
necessary to enlist help in the form of other instructors, potentially
increasing bias in evaluation. Oral exams that, normally, range from
15-45 minutes long \textbf{citations} may need to be shortened, which
will impact reliability negatively (Memon, Joughin, and Memon 2010).
Where professors could administer these exams in person, they may
instead need to administer them on a computer, using video calling
software like zoom, which can enlist a host of new issues to deal with.

With scale comes a new group to study beyond the students: the
administrators of the exam. This study follows a group of graduate
teaching assistants who are the instructor of record for one-credit
programming courses. While seasoned instructors may feel comfortable
engaging in a conversation with students and gauging understanding,
novice instructors administering oral exams is a very different
proposition. This study attempts to understand problems of practice that
arise for novice instructors when attempting to scale oral exam
administration to a large number of students.

**Tie in looking into instructors.**

\hypertarget{methods}{%
\subsection{Methods}\label{methods}}

This study follows six graduate teaching assistants that were
instructors of record for one-credit programming courses. These courses
cover either R programming or SAS programming. Each instructor had three
or four sections of one of these courses with about 35 students per
section.

\hypertarget{sas-course-information}{%
\subsubsection{SAS Course Information}\label{sas-course-information}}

This course introduces students to programming in SAS through SAS Studio
in the SAS OnDemand for Academics browser-based platform. The course
covers topics like reading in raw data with \texttt{PROC\ IMPORT}, basic
row and column manipulations of SAS datasets through the
\texttt{DATA\ step}, summarizing data numerically with
\texttt{PROC\ FREQ}, \texttt{PROC\ UNIVARIATE}, and
\texttt{PROC\ MEANS}, summarizing data graphically with
\texttt{PROC\ SGPLOT} and \texttt{PROC\ SGPANEL}, analysis of means
using \texttt{PROC\ TTEST} and \texttt{PROC\ GLM}, and fitting linear
models (one-way ANOVA and Multiple Linear Regression) through
\texttt{PROC\ GLM}.

The course has a prerequisite of a business statistics course or a
corequisite of a second course in statistics. These restrictions allow
the course to cover the creation and interpretation of statistical
models without the need to introduce the statistical concepts
surrounding these topics.

The course serves two major audiences:

\begin{itemize}
\tightlist
\item
  Statistics majors and minors
\item
  Business majors
\end{itemize}

The statistics majors generally have taken an introductory statistics
course, an introductory programming course in python, and are taking the
second course in statistics concurrently with the course (or have
already taken it). The statistics minors are similar but do not
generally have the introductory programming course. The business majors
usually enroll in the course having taken a business statistics course
at some point earlier in their academic careers.

For both audiences, the broad purpose of the course from the program
level is to provide the students with a thorough introduction to using
SAS in order to facilitate the use of SAS in their upper level
statistics or business courses. The course learning outcomes are listed
on the syllabus as follows:

\begin{quote}
This course provides an introduction to programming using the SAS
statistical software. At the end of this course students will be able to

\begin{itemize}
\tightlist
\item
  write and explain the purpose of basic SAS steps and statements
\item
  utilize SAS help files to customize SAS programs
\item
  correctly read well-structured data into SAS
\item
  perform common data manipulations using SAS
\item
  produce and interpret standard numeric and graphical summaries of data
  using SAS
\item
  produce, interpret, and assess the validity of assumptions of
\item
  statistical hypothesis tests and confidence intervals using SAS
\end{itemize}
\end{quote}

\hypertarget{r-course-information}{%
\subsubsection{R Course Information}\label{r-course-information}}

This course introduces students to programming in the R software through
the RStudio Interactive Development Environment. The course covers
topics like using and manipulating common R objects (such as lists, data
frames, and (atomic) vectors), reading in raw data using the
\texttt{readr}, \texttt{haven}, and \texttt{readxl} packages, creating
output documents with R Markdown, common row and column manipulations
with the \texttt{dplyr} and \texttt{tidyr} packages, summarizing data
numerically, summarizing data graphically with the \texttt{ggplot2}
package, using vectorized functions and for loops, and writing custom
functions.

This course does not have a prequisite or corequisite. This causes the
course to cover more programming-centric topics and less statistical
topics when compared to the SAS programming course.

The course serves the same two major audiences as the SAS programming
course (statistics majors/minors and business majors). Again, the
statistics majors have generally had an introductory programming course
prior to taking this R programming course but the other students usually
do not have a programming background.

The broad purpose of the course from the program level is to provide the
students with a thorough introduction to using R in order to facilitate
the use of R in their upper level statistics or business courses. The
course learning outcomes are listed on the syllabus as follows:

\begin{quote}
This course provides an introduction to programming using the
statistical software R. At the end of this course students will be able
to

\begin{itemize}
\tightlist
\item
  read, write, and explain the steps and purpose of R programs
\item
  efficiently read in and manipulate data in R
\item
  utilize R help and other resources to customize programs
\item
  produce and interpret static and interactive graphics using R
\end{itemize}
\end{quote}

\hypertarget{course-structure}{%
\subsubsection{Course Structure}\label{course-structure}}

These programming courses have a flipped structure where students
receive the course content prior to their class meeting time in order to
spend most of the class time on activities involving active
participation. With many of the students in the courses having limited
to no programming exposure, having a flipped course allows the students
to gain basic knowledge of the material through videos. They can then
come to class and actually \textbf{do} programming while having an
expert (the course instructor) in the room and support from their peers.
This ideally leads to students having a direct line of help when they
get stuck rather than running into an error, getting frustrated, and
stopping their learning. They also can see the struggles of others to
know that they aren't out of place if they are having difficulty. Not
only that but students can go back to rewatch topics they find
difficult.

Although not the purpose of using a flipped structure, another benefit
of this design is that graduate students can reasonably be expected to
support students within the classroom as the instructor of the course.
These graduate students may not have the pedagogical knowledge or
experience to effectively teach programming material themselves, but
many have the problem solving skills to help students work through
issues that come up. As these two courses serve approximately 700-800
students each semester, this is a definite benefit from the
administrative side. Graduate students can be assigned to reasonably
small sections of students (40 students maximum) and provide more
one-on-one attention to the learners.

\textbf{\emph{Not sure if these next four paragraphs are needed}} These
classes meet once a week for 50 minutes at a time. Most weeks students
are expected to watch one to two videos that cover the learning
objectives for the week. These videos are approximately 10-20 minutes
each and are created by the course coordinator. The videos usually have
two or three quizzes throughout, each with three to four questions, to
help students assess their understanding as they watch. Students are
expected to complete these videos prior to their weekly session.

During most class periods, the instructor briefly recaps the material
for the week (less than five minutes), takes questions, and introduces
the in-class activity for the week. The students are then given the bulk
of the class time to work in small groups or individually on the
activity. The instructor takes an active role during this period,
checking in on students and answering questions that come up. The last
five minutes of the class are used to recap important parts of the
activity.

Weeks that follow this structure then have a quiz administered on the
course learning management software that must be completed a few days
after the class. The questions in these quizzes vary across the common
auto-graded types of questions such as multiple choice, fill in the
blank, etc. and some are very similar to the questions asked during the
videos.

There were four weeks that did not follow this structure. Two of those
weeks replaced the usual in-class period with a short quiz (10 minutes)
followed by time to work on a homework assignment. These assignments
involved creating their own original program to answer questions of
interest along with finding a data set of their own and applying class
concepts.

The remaining two weeks were dedicated to administering oral exams.
These weeks involved no new content and no class was held. Each oral
exam accounted for eight percent of the students' overall course grade.

Each section of the course had a maximum of 40 students. Most sections
had between 35 and 40 students but a few had much lower enrollment. In
total, there were nearly 700 students across all of the sections of the
two courses. Three instructors had four sections and administered oral
exams to roughly 140 students each. Three instructors had three sections
each but these tended to be sections with slightly lower enrollment. Two
of these instructors administered oral exams to roughly 90 students. One
instructor had a teaching assistant appointment for less time so only
administered exams to two of their sections (roughly 70 students). The
course coordinator administered oral exams to the remaining section of
students.

\hypertarget{designing-the-oral-discussions}{%
\subsubsection{Designing the Oral
Discussions}\label{designing-the-oral-discussions}}

As oral exams are not common for courses taken by these students, the
oral exam was rebranded as an `oral discussion' to hopefully lessen the
anxiety students felt.

The design of the oral discussions roughly followed that of a structured
interview protocol ({``Structured Interviews,''} n.d.). In this
structure, subject matter experts create relevant questions and a rating
scale for responses. The interviewers systematically ask these questions
of the interviewees in order to be as fair as possible while ensuring
valid and reliable ratings. Follow up or `probe' questions are also
created to help elicit appropriate responses at the level desired by the
interviewer.

The first oral discussion occurred in week six of the R course and week
seven of the SAS course. For the first oral discussion, the course
coordinator developed an example program and a script of candidate
questions to ask about the program. They also created a list of probable
student responses with subsequent follow up questions to gain the
appropriate clarity of responses. The candidate questions were split
into two categories: higher-level questions involving more detailed
explanations and lower-level questions involving mostly recall. As the
oral discussions were slated to be taken in a five minute window, two of
each type of question were developed for the first oral discussion. The
grading scales were 0, 1, 2, or 3 points, and 0, 1, or 2 points for the
higher-level and lower-level questions, respectively.

During a weekly meeting of all instructors, the instructors and course
coordinators went through the example programs (one SAS program and one
R program) and questions written by the course coordinator. The
instructors then split into pairs (one a SAS instructor and one an R
instructor) and practiced administering the discussion to the other
instructor. Using this as feedback, the questions and follow ups were
modified as needed.

For the second oral discussion, occuring in week 12 for both courses,
the course coordinator developed an example program to share with
students and a similar program to use for the actual oral discussion.
This time a weekly meeting was devoted to having the instructors of the
courses develop questions and the corresponding follow ups. The
instructors were again split into pairs and practiced administering the
discussions.

After the first oral discussions, the instructors felt strongly that
they would like to have a finer scale for their grading rubric. Whereas
before the lower-level questions were out of 0, 1, or 2 scale, these
were now out of a 0, 1, \ldots, 4 scale. Similarly, the higher-level
questions were now graded on a scale of 0, 1, \ldots, 6.

The oral discussion scripts used by the instructors and SAS or R
programs are available in the appendix.

\hypertarget{administering-the-oral-discussions}{%
\subsubsection{Administering the Oral
Discussions}\label{administering-the-oral-discussions}}

The oral discussions were administered through zoom. This decision was
made for the ease of scheduling on both the instructor and student side.
As students and instructors each had their own class schedules to
contend with, having time slots spread throughout the week without the
need for room reservations seemed like the most appropriate course of
action. The discussions were set to five minutes in length with the
intention that the actual time for the discussions would be closer to
three-four minutes, allowing for a buffer between appointments. The
discussions were closed notes and did not involve students coding. They
were instructed to be prepared to discuss code only. Instructors were
encouraged to build in down time for their appointments to deal with any
appointments that went long or had technical difficulty. For instance,
only making appointments available from the start of the hour until 50
minutes past. This would leave 10 minutes to account for issues that may
arise.

The weeks prior to the discussions students were able to use an online
scheduler to sign up for a five minute time slot. They were instructed
to sign up for a time slot by the Wednesday prior to discussion week and
contact their instructor if none of the designated time slots worked for
them. If this wasn't followed, a 25\% deduction to their score could be
given (this did not end up being used for any students). Students with
accommodations were to contact their instructor to make sure they were
able to have their accommodations met.

The week prior to the oral discussion, students were provided an example
program, the rubric used for grading the discussions, and the
instructions for how the oral discussion would progress. For the first
oral discussion, we chose to use the same program as both the example
program and the actual program used in the oral discussion in order to
ease student anxiety. As students were more comfortable in the second
round of oral discussions, different programs were used. The rubric used
was based on that given by (Theobold 2021), and was modified between the
first discussion and second discussion based on instructor feedback. The
instructions for the discussion itself are given below:

\begin{itemize}
\tightlist
\item
  We want to ask you a few questions about a SAS program (or an R
  Markdown document) we'll share with you.
\item
  I'll share my screen, ask you to consider particular pieces of code
  and describe to me what that code does or why we might run it.
\item
  I may ask clarification questions or follow-up questions if you don't
  fully answer the question.
\item
  If you don't know the answer, that's ok. Just let us know and we'll
  move to the next item.
\item
  We do have firm time limits on answers to questions. We may have to
  cut you off so we can get all of the questions in a timely manner.
\item
  Any questions?
\end{itemize}

Students were instructed to log into the zoom meeting a few minutes
prior to their designated time slot. The zoom meetings themselves
utilized the waiting room feature. This ensured that students would not
pop into the meeting and interrupt an ongoing discussion. Students were
required to have their camera on during the discussions. Students are
not required to have a laptop at this institution but no instructors
reported issues with students not being able to accommodate virtual
meetings with a webcam (this may be due to the excellent library
facilities on campus).

The rubric for the oral discussion was set up on the learning management
system allowing the instructors to provide their graded feedback as the
discussion were taking place. These grades were not released until all
students had completed their discussions.

The discussions themselves were recorded so that any disagreements about
grading between the instructors and students could be mediated by the
course coordinator.

Lastly, some instructors found that they needed to have extra `hidden'
times on the last day of the week to account for students that had
missed their appointments due to personal or technological reasons. The
number of students needing this extra period was pretty low for all
instructors.

\hypertarget{monitoring-instructor-sentiment-and-issues}{%
\subsubsection{Monitoring Instructor Sentiment and
Issues}\label{monitoring-instructor-sentiment-and-issues}}

In order to understand problems that arose, and sentiment around,
administering the oral discussions, the course coordinator requested
feedback at various points of the process formally and informally.

Initial thoughts about giving, and experience with, oral exams were
requested on a Google form prior to administering the discussions. The R
course had its first oral discussion one week prior to the SAS course.
Reflections and advice were discussed in the weekly meeting and recorded
for future reference. The meeting and conversations after the first SAS
course oral discussion was likewise recorded. Another Google form was
administered at this point as well. The second oral discussions occurred
during the same week for both courses. The meeting and conversations the
week after administration were recorded. Lastly, a Google form was given
to record their final thoughts on administering oral discussion in the
course. All of the forms are available in the appendix.

\textbf{Matthew} will work on the methodology of analyzing the data.

The purpose of analyzing the data was to categorize common topics of
discussion that reoccurred throughout the oral exam process, and record
whether evidence came from a TA discussion or a survey. In the first
pass, five tentative categories were created, with evidence for each
marked based on if the data came from a discussion or survey. In the
second and third passes, the categories were refined after recognizing
that initial themes may be better represented by including more
sub-themes. For example, instead of discussion of time being a theme,
also including separate aspects of time that were focused on. Finally,
we looked over which categories had sufficient support to be included.
They were included if they had 5 or more pieces of evidence.

\hypertarget{results}{%
\subsection{Results}\label{results}}

\hypertarget{the-tas-focused-on-time}{%
\subsubsection{The TAs focused on time}\label{the-tas-focused-on-time}}

\textbf{The Quantity of time}

On a pre-exam survey, half of the TAs expressed concerns about the
``time'' or ``timeframe'' of administering so many oral exams to so many
students over a week. After the first round, some of the TAs gave
estimates of 12 to 18 hours to complete their oral exams.

Others used verbal descriptions, such as it ``took way too long'' and
``overall, very time consuming'' to express the magnitude of the time
commitment. Words such as ``grueling'', that this was something they had
to ``endure'', and that it was ``extremely time inefficient'' were used
to express their feelings towards the amount of time needed. One TA,
after administering their first oral exam, said (somewhat jokingly), ``I
was losing my sanity being on zoom for so long''.

One reason for this focus on the time commitment was given by one TA,
who commented that they were promised less work as first years, but that
this was not the case in this class.''If the oral exam format goes
unchanged'', they said, ``I am unsure if I am fit to be an instructor
next semester''.

\textbf{What they were doing during that time}

Beyond the actual time it took to administer the exams was talk centered
around what they could and couldn't do during that time. In the first
whole group discussion after the first oral exam, several people joked
about not even being able to use the bathroom since you have to be
``100\% focused''. In a later discussion, they focused on incorporating
time to get a snack, stand up, and walk around.

The way they were spending their time administering exams was contrasted
with HW grading. In one discussion, one TA commented that, for them, the
exams took longer, and, unlike HW, with the exams you had to ``block out
time''. Another TA disagreed, saying the time commitment felt similar,
but later, acknowledging that it could take similar time, a final TA
commented that grading HW was more ``flexible''.

\textbf{When things went wrong with timing}

Especially after the first few exams, TAs focused on issues with student
timing. One talked about the fragility of the online set up, where if
even one student comes late, it can ``ruin the timing for all of the
students''. This inspired another strong question about ``how to make
everyone register and show up on time''. Having to reschedule with
students who missed, either to Saturday or to next week,and how to avoid
it, dominated the final group discussion after the administration of the
last oral exam. Other issues of timing were caused by technical
difficulties with Zoom and the ensuing delays, as well as the burden of
time bleeding into other classes and exams the TAs had outside of their
positions.

When the TAs were asked about advice they would give to future TAs
participating in the same experience as themselves, their advice
overwhelmingly centered around optimal time management due to previously
mentioned experiences with students not showing up on time or having to
reschedule. For example, TAs offered comments such as ``If possible,
avoid scheduling any time on Friday'' so as to plan for students who
miss their time slots, ``try to make everyone schedule and show up on
time'', and ``build in more than a 5 minute break every 30 minutes'' or
else, due to students not arriving to the meeting on time, ``you'll need
to use {[}it{]}''.

\hypertarget{the-tas-focused-on-equity}{%
\subsubsection{The TAs focused on
Equity}\label{the-tas-focused-on-equity}}

Even before administering exams, TAs worried about their role not just
as administrators but as graders. One worried that the grading might be
``too subjective''. Another worried about their ``poor memory'' and bias
that could come from familiarity. One TA even cautioned against ``food
bias'' where you many grade someone more harshly while hungry. Some of
these issues were borne out during the first discussion when two TAs
used different follow-up questions other than on the rubric when asking
about an R object, and the teacher and one TA used different follow up
questions to discuss R libraries. Several TAs after the first few oral
exams expressed interest in changing the grading scale, so as to be fair
to students who may have needed prompting, but still got the right
answer.

The TAs were also worried that the oral exam would be confounded with
speaking ability. In one exchange between the teacher and one TA, the TA
shared one ESL student wanted to do it in person since, through zoom,
``it {[}was{]} hard for them to understand the questions clearly''.
Other TAs during the first survey expressed similar views, with one on
the final survey indicating that ESL students should be given more time,
since ``this is not an english test''.

There were other minor focuses that didn't always consistently appear or
only appeared because a they were explicitly asked in a survey: their
views on oral vs written exams, and positive thoughts (if any) about the
process.

\hypertarget{their-views-on-oral-vs-written-exams}{%
\subsubsection{Their views on Oral vs Written
Exams}\label{their-views-on-oral-vs-written-exams}}

After the first round of exams, a few TAs expressed doubt over the
utility of the oral exam format. ``Not sure what this accomplishes over
HW/{[}online{]} quiz'' on said. ``A paper quiz'', another said, ``could
serve the same goal''. After seeing these responses, a specific question
about their views on oral vs written exams was asked, with mixed
responses. Half agreed there was something different or better, but
those differences boiled down to ``open book'' vs ``closed book'' exams,
examining ``on the fly knowledge''. Luckily one TA did mention the
benefit of being able to ``follow up''. The other half thought that a
well crafted, open ended quiz could essentially serve the same purpose.

\hypertarget{positives-or-oral-exam-experience}{%
\subsubsection{Positives or Oral exam
experience}\label{positives-or-oral-exam-experience}}

When asked directly about positives, only half of the TAs had something
to say, with all the comments being centered around having increased
student interactions where, in such a large class, they usually didn't.
One TA commented that, unprovoked, a few students talked about liking
the format.

\hypertarget{discussion}{%
\subsection{Discussion}\label{discussion}}

\textbf{Matthew and Justin} will work on this part after literature
review and results are complete.

This is the first large scale oral exam study done in the wider oral
exam literature, and the first to examine how trying to apply oral
assessment at a larger scale works within statistics education.

Unlike previous studies, this study had \textbf{this many participants}.
However, it involved conducting oral exams using realistic resources,
necessitating the reduction of oral exam times (5 minutes) and
increasing the number of instructors (6 TAs, 1 Professor). Since in the
large scale scenario TAs become essential to successful implementation
of an oral exam, we focused on their thoughts and feelings.

From their perspective, the oral exam format was probably a failure.
Most of the topics centered around being able to survive administering
them, as well as questioning their effectiveness in comparison to more
standard assessments. \textbf{Students see written exam as gold standard
citation?}.

On a positive note, these TAs were aware of and sensitive to potential
biases and equity issues, and voiced their opinions consistently.

Overall, we would suggest that under realistic circumstnaces,
administering oral exams at larger scales may greatly weaken what makes
oral exams important/interesting/powerful in the first place. With the
necessary shortenting of their administartion comes a loss in
reliability ((Memon, Joughin, and Memon 2010)).

Probably will mention this not being a good idea, probably losing what
makes an Oral exam really good as we try to adapt to scale

Probably going to talk about how this is our attempt in a realistic
setting\ldots.we could try to create an ideal one, and see, or we could
couch it in based on the literature, ideally it would look like this.
See that it's really not possible, this was our attempt realistically.

One big issue is most oral exams are in a problem based format, some
questions, time\ldots.with only 5 minutes, you can't do that or do that
well. You lose that part of really testing a student's thinking and
assessing how well they can explain \emph{and} do.

Issues with reliability.

\textbf{Future research questions} include

\begin{itemize}
\tightlist
\item
  AI enhanced oral assessment?
\item
  Student perceptions positive in literature, but how does doing it for
  5 minutes in zoom change things? Having to compare and contrast
  experiences
\item
  \textbf{Others?}
\end{itemize}

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

\textbf{Matthew and Justin} will work on this part after literature
review and results are complete.

\hypertarget{references}{%
\subsection*{References}\label{references}}
\addcontentsline{toc}{subsection}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-Asklund2003}{}}%
Asklund, Ulf, and Lars Bendix. 2003. {``Oral Vs. Written Evaluation of
Students.''} \emph{Pedagogisk Inspirationskonferens, Lunds Tekniska
Högskola, Sid}, 45--46.

\leavevmode\vadjust pre{\hypertarget{ref-Beccaria2013}{}}%
Beccaria, Gavin. 2013. {``The Viva Voce as an Authentic Assessment for
Clinical Psychology Students.''} \emph{Australian Journal of Career
Development} 22: 139--42.

\leavevmode\vadjust pre{\hypertarget{ref-carver2016guidelines}{}}%
Carver, Robert, Michelle Everson, John Gabrosek, Nicholas Horton, Robin
Lock, Megan Mocko, Allan Rossman, et al. 2016. {``Guidelines for
Assessment and Instruction in Statistics Education (GAISE) College
Report 2016.''}

\leavevmode\vadjust pre{\hypertarget{ref-Huxham2012}{}}%
Huxham, Mark, Fiona Campbell, and Jenny Westwood. 2012. {``Oral Versus
Written Assessments: A Test of Student Performance and Attitudes.''}
\emph{Assessment \& Evaluation in Higher Education} 37: 125--36.

\leavevmode\vadjust pre{\hypertarget{ref-Iannone2012}{}}%
Iannone, Paola, and A Simpson. 2012. {``Oral Assessment in Mathematics:
Implementation and Outcomes.''} \emph{Teaching Mathematics and Its
Applications: International Journal of the IMA} 31: 179--90.

\leavevmode\vadjust pre{\hypertarget{ref-Joughin2010}{}}%
Joughin, Gordon. 2010. \emph{A Short Guide to Oral Assessment}. Leeds
Met Press in association with University of Wollongong.

\leavevmode\vadjust pre{\hypertarget{ref-Memon2010}{}}%
Memon, Muhammed Ashraf, Gordon Rowland Joughin, and Breda Memon. 2010.
{``Oral Assessment and Postgraduate Medical Examinations: Establishing
Conditions for Validity, Reliability and Fairness.''} \emph{Advances in
Health Sciences Education} 15: 277--89.

\leavevmode\vadjust pre{\hypertarget{ref-Newell2023}{}}%
Newell, Samantha J. 2023. {``Employing the Interactive Oral to Mitigate
Threats to Academic Integrity from ChatGPT.''} \emph{Scholarship of
Teaching and Learning in Psychology}.

\leavevmode\vadjust pre{\hypertarget{ref-Ohmann2019}{}}%
Ohmann, Peter. 2019. {``An Assessment of Oral Exams in Introductory
Cs.''} In \emph{Proceedings of the 50th ACM Technical Symposium on
Computer Science Education}, 613--19.

\leavevmode\vadjust pre{\hypertarget{ref-Ramella2019}{}}%
Ramella, Daniele. 2019. {``Oral Exams: A Deeply Neglected Tool for
Formative Assessment in Chemistry.''} In \emph{Active Learning in
General Chemistry: Specific Interventions}, 79--89. ACS Publications.

\leavevmode\vadjust pre{\hypertarget{ref-SIP}{}}%
{``Structured Interviews.''} n.d.
\url{https://www.opm.gov/policy-data-oversight/assessment-and-selection/structured-interviews/}.

\leavevmode\vadjust pre{\hypertarget{ref-Theobold2021}{}}%
Theobold, Allison S. 2021. {``Oral Exams: A More Meaningful Assessment
of Students' Understanding.''} \emph{Journal of Statistics and Data
Science Education} 29: 156--59.

\end{CSLReferences}



\end{document}
